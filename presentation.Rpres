Multinomial Logistic Regression in R
========================================================
author: Sára Mód & Jasper Ginn
date: 26/09/2018
autosize: true
font-family: 'Helvetica'
css: custom.css

Table of Contents 
========================================================

- Multinomial logistic regression: when to use?
- Introduction to practical
- Bias-variance trade-off
- Understanding v. predicting
- A 'naive' multinomial model
- Cross-validation
- Regularization
- Appendices

* Presentation: http://rpubs.com/jhginn/mvsuu

Multinomial logistic regression: when to use? (1)
========================================================

<center><img src="s1.png"></center>

Multinomial logistic regression: when to use? (2)
========================================================

<center><img src="sara2.png"></center>

Multinomial logistic regression: when to use? (3)
========================================================

<center><img src="sara3.png"></center>

Multinomial logistic regression: when to use? (4)
========================================================

<center><img src="sara4.png"></center>

Introduction to practical
========================================================

Execute the following in a terminal:

```shell
docker pull jhginn/multivariate_statistics_uu
```

Then:

```shell
docker run -e PASSWORD=stats -p 8787:8787 jhginn/multivariate_statistics_uu
```

Go to [http://localhost:8787](http://localhost:8787)

OR
========================================================

* Visit: http://maraudingmeerkat.nl/practical/yourfirstname/

  * Username: rstudio
  * Password: stats
 
Bias-Variance Trade-Off 
========================================================

$$
Total\:error = Bias + Variance + Var(\epsilon)
$$

<center><img src="bvar.png"></center>

Understanding v. Prediction
========================================================

<img src="understandpredict.png">

A 'naive' model
========================================================

<img src="model.png">

Simple Cross-validation
========================================================

<img src="cv.png">

Regularization
========================================================

- Log-likelihood function is what we try to optimize over successive iterations of the algorithm
  - Intuition: the outcomes must be *likely* given the data
  
  $$\mathcal{L}(\hat{y}, y) = y \log(\hat{y}) + (1-y) \log(1-\hat{y}) \\$$

- We try to minimize the log-likelihood through successive iterations. 
- Much like we do with Newton-Rhapson

Intuition
========================================================

- How? Add a penalty term to likelihood so we don't overfit!
  - Aim:
      - Introduce more bias / restrict variance
      - Provide more generalizable results
- By increasing the regularization parameter $\lambda$, we 'increase' the minimum cost.

***

```{r, echo=FALSE}
# Create some ys
x <- seq(-5,5,0.01)
y <- 3*x^2 + 2*x -2
y2 <- 3*x^2 + 2*x +20
y3 <- 3*x^2 + 2*x +40

df <- data.frame(x=x, y1=y, y2=y2, y3=y3)
df <- reshape2::melt(df, 1)
library(ggplot2)
ggplot(df, aes(x=x, y=value, color=variable)) +
  geom_line(size=2) +
  scale_color_discrete(labels = c("no lambda", "lambda = 10", "lambda = 20"),
                       name = "Lambda") +
  theme_bw() +
  theme(axis.text = element_blank(),
        axis.ticks = element_blank(),
        axis.line = element_blank(),
        axis.title = element_blank(),
        legend.text = element_text(size=15),
        legend.title = element_text(size=18))
```
 
Appendix: design matrix and responses
=======================================================

$$
y = \begin{bmatrix}
y_0 \\
\vdots \\
y_m
\end{bmatrix}, \: y_i \in \{1, 0\} \\
X = \begin{bmatrix}
x_1 & x_2 & \dots & x_n
\end{bmatrix}, \: \dim(X) = (m, n)
$$

Appendix: Log-likelihood & L2-Norm 
=======================================================

$$
w = \begin{bmatrix}
w_1 \\
\vdots \\
w_n
\end{bmatrix}, \: b \in \mathbb{R}, \:
\hat{y} = \sigma(wX^{T} + b) \\
\mathcal{L}(\hat{y}, y) = y \log(\hat{y}) + (1-y) \log(1-\hat{y}) \\
\mathcal{J}(w, b) = -\frac{1}{m} \sum_{i=1}^m \mathcal{L}(\hat{y}, y) + \frac{\lambda}{2m} ||w||^2_2 \\
||w||^2_2 = w^{T} \cdot w, \: \lambda \in \mathbb{R}
$$

Appendix: pseudo-code
=======================================================

```
## Set parameters w and b to 0
w = matrix(0L, ncol=ncol(X))
b = 0
## Update parameters
for i in max_iterations:
  ## Linear combination & sigmoid function
  model = sigmoid(w %*% t(X) + b)
  ## Compute cost
  cost = -(1/m) * sum(y*log(yhat) + (1-y) * log(1-yhat)) + (lambda/2m * norm(w))
  ## Compute gradients
  dw = (1/m) * (t(X) %*% matrix(A-Y, ncol=1)) + t((lambda / m) * w)
  db = (1/m) * sum(A-Y)
  ## Update parameters
  w = w - t(learning_rate * dw)
  b = as.vector(b - t(learning_rate * db))
```

Appendix: Links
=======================================================

- [`mnlr` R package](https://github.com/JasperHG90/mnlr) used for multinomial logistic regression
- [github repository with slides](https://github.com/JasperHG90/multivariate_statistics_uu_presentation)
- [github repository with practical & Dockerfile](https://github.com/JasperHG90/multivariate_statistics_uu)
- [Docker image on Dockerhub](https://hub.docker.com/r/jhginn/multivariate_statistics_uu/)

Appendix: Resources
=======================================================

- [Introduction to statistical learning](https://www-bcf.usc.edu/~gareth/ISL/)
  - Bias-variance trade-off (p.30)
  - Simple cross-validation (p.176)
  - Shrinkage methods (pp.214-227)
- [Docker documentation](https://docs.docker.com/)
- [Logistic Regression in R](https://rpubs.com/rslbliss/r_logistic_ws)
